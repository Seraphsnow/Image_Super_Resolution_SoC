{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0KcTn0mUXxZ",
        "outputId": "442006b7-e8ac-4115-a64c-5accb9f94a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cats.zip\n",
            "replace cats/27.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: cats/27.jpg             \n",
            "  inflating: cats/28.jpg             \n",
            "  inflating: cats/29.jpg             \n",
            "  inflating: cats/30.jpg             \n",
            "  inflating: cats/31.jpg             \n",
            "  inflating: cats/32.jpg             \n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "!unzip /content/cats.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Check device\n",
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10  # Increase the number of epochs\n",
        "batch_size = 32  # Increase the batch size\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "classes = ('cats', 'dogs')\n",
        "\n",
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),  # Use random resized crop instead of resize\n",
        "    transforms.RandomHorizontalFlip(),  # Add random horizontal flip for data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load train and test datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder('/content/train', transform=transform)\n",
        "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = torchvision.datasets.ImageFolder('/content/test', transform=transform)\n",
        "test_loader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 220)\n",
        "        self.fc2 = nn.Linear(220, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create the model instance and move it to the device\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)  # Use Adam optimizer\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "with th.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    class_correct = [0 for _ in range(num_classes)]\n",
        "    class_total = [0 for _ in range(num_classes)]\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs= model(images)\n",
        "        _, predicted = th.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i].item()\n",
        "            pred = predicted[i].item()\n",
        "            class_correct[label] += int(label == pred)\n",
        "            class_total[label] += 1\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Overall Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        class_accuracy = 100 * class_correct[i] / class_total[i]\n",
        "        print(f'Accuracy of {classes[i]}: {class_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJARgaK4XAHi",
        "outputId": "e63c8a82-c803-482b-f5aa-82ff518fa524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 59.29%\n",
            "Accuracy of cats: 42.86%\n",
            "Accuracy of dogs: 75.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image\n",
        "paths = ('/content/cats/2016.jpg','/content/cats/27.jpg','/content/cats/28.jpg',\n",
        "         '/content/cats/29.jpg','/content/cats/30.jpg','/content/cats/31.jpg',\n",
        "         '/content/cats/32.jpg','/content/dog1.jpg')\n",
        "for i in range(len(paths)):\n",
        "  img = read_image(paths[i])\n",
        "  img = transforms.ToPILImage()(img)\n",
        "  p = transform(img)\n",
        "  p.size()\n",
        "  os = model(p)\n",
        "  # max returns (value ,index)\n",
        "  _, predicted = th.max(os, 1)\n",
        "  predicted_label = classes[predicted]\n",
        "  print(predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inOiivjeX64j",
        "outputId": "d5f5e7b9-e213-4fbb-fcfc-55453e9701d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dogs\n",
            "dogs\n",
            "cats\n",
            "cats\n",
            "cats\n",
            "dogs\n",
            "cats\n",
            "dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tCm2s9PfJ4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}