{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoyWMXkemGuG",
        "outputId": "80596017-53b8-4bc2-b091-2b4a810e57b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 28, 28])\n",
            "Epoch [1/10], Step [2000/15000], Loss: 0.1327\n",
            "Epoch [1/10], Step [4000/15000], Loss: 0.0060\n",
            "Epoch [1/10], Step [6000/15000], Loss: 0.0087\n",
            "Epoch [1/10], Step [8000/15000], Loss: 0.0523\n",
            "Epoch [1/10], Step [10000/15000], Loss: 0.0019\n",
            "Epoch [1/10], Step [12000/15000], Loss: 1.1194\n",
            "Epoch [1/10], Step [14000/15000], Loss: 1.2416\n",
            "Epoch [2/10], Step [2000/15000], Loss: 0.0000\n",
            "Epoch [2/10], Step [4000/15000], Loss: 0.0006\n",
            "Epoch [2/10], Step [6000/15000], Loss: 0.0804\n",
            "Epoch [2/10], Step [8000/15000], Loss: 0.0004\n",
            "Epoch [2/10], Step [10000/15000], Loss: 0.0007\n",
            "Epoch [2/10], Step [12000/15000], Loss: 0.1645\n",
            "Epoch [2/10], Step [14000/15000], Loss: 0.0268\n",
            "Epoch [3/10], Step [2000/15000], Loss: 0.0002\n",
            "Epoch [3/10], Step [4000/15000], Loss: 0.0044\n",
            "Epoch [3/10], Step [6000/15000], Loss: 0.0011\n",
            "Epoch [3/10], Step [8000/15000], Loss: 0.0002\n",
            "Epoch [3/10], Step [10000/15000], Loss: 0.0100\n",
            "Epoch [3/10], Step [12000/15000], Loss: 0.0003\n",
            "Epoch [3/10], Step [14000/15000], Loss: 0.0001\n",
            "Epoch [4/10], Step [2000/15000], Loss: 0.0000\n",
            "Epoch [4/10], Step [4000/15000], Loss: 0.0001\n",
            "Epoch [4/10], Step [6000/15000], Loss: 1.0436\n",
            "Epoch [4/10], Step [8000/15000], Loss: 0.0021\n",
            "Epoch [4/10], Step [10000/15000], Loss: 0.0001\n",
            "Epoch [4/10], Step [12000/15000], Loss: 0.0136\n",
            "Epoch [4/10], Step [14000/15000], Loss: 0.0000\n",
            "Epoch [5/10], Step [2000/15000], Loss: 0.0954\n",
            "Epoch [5/10], Step [4000/15000], Loss: 0.0001\n",
            "Epoch [5/10], Step [6000/15000], Loss: 0.0001\n",
            "Epoch [5/10], Step [8000/15000], Loss: 0.0001\n",
            "Epoch [5/10], Step [10000/15000], Loss: 0.0000\n",
            "Epoch [5/10], Step [12000/15000], Loss: 0.0000\n",
            "Epoch [5/10], Step [14000/15000], Loss: 0.0006\n",
            "Epoch [6/10], Step [2000/15000], Loss: 0.2325\n",
            "Epoch [6/10], Step [4000/15000], Loss: 0.0000\n",
            "Epoch [6/10], Step [6000/15000], Loss: 0.0164\n",
            "Epoch [6/10], Step [8000/15000], Loss: 0.0001\n",
            "Epoch [6/10], Step [10000/15000], Loss: 0.0003\n",
            "Epoch [6/10], Step [12000/15000], Loss: 0.0010\n",
            "Epoch [6/10], Step [14000/15000], Loss: 0.0094\n",
            "Epoch [7/10], Step [2000/15000], Loss: 0.0002\n",
            "Epoch [7/10], Step [4000/15000], Loss: 0.0001\n",
            "Epoch [7/10], Step [6000/15000], Loss: 0.0000\n",
            "Epoch [7/10], Step [8000/15000], Loss: 0.0001\n",
            "Epoch [7/10], Step [10000/15000], Loss: 0.0000\n",
            "Epoch [7/10], Step [12000/15000], Loss: 0.0002\n",
            "Epoch [7/10], Step [14000/15000], Loss: 0.0000\n",
            "Epoch [8/10], Step [2000/15000], Loss: 0.0000\n",
            "Epoch [8/10], Step [4000/15000], Loss: 0.0057\n",
            "Epoch [8/10], Step [6000/15000], Loss: 0.0006\n",
            "Epoch [8/10], Step [8000/15000], Loss: 0.0000\n",
            "Epoch [8/10], Step [10000/15000], Loss: 0.0002\n",
            "Epoch [8/10], Step [12000/15000], Loss: 0.0001\n",
            "Epoch [8/10], Step [14000/15000], Loss: 0.0000\n",
            "Epoch [9/10], Step [2000/15000], Loss: 0.0000\n",
            "Epoch [9/10], Step [4000/15000], Loss: 0.0000\n",
            "Epoch [9/10], Step [6000/15000], Loss: 0.0000\n",
            "Epoch [9/10], Step [8000/15000], Loss: 0.0000\n",
            "Epoch [9/10], Step [10000/15000], Loss: 0.3621\n",
            "Epoch [9/10], Step [12000/15000], Loss: 0.3375\n",
            "Epoch [9/10], Step [14000/15000], Loss: 0.0000\n",
            "Epoch [10/10], Step [2000/15000], Loss: 0.0003\n",
            "Epoch [10/10], Step [4000/15000], Loss: 0.0093\n",
            "Epoch [10/10], Step [6000/15000], Loss: 0.0002\n",
            "Epoch [10/10], Step [8000/15000], Loss: 0.0000\n",
            "Epoch [10/10], Step [10000/15000], Loss: 0.0000\n",
            "Epoch [10/10], Step [12000/15000], Loss: 0.0023\n",
            "Epoch [10/10], Step [14000/15000], Loss: 0.0001\n",
            "Finished Training\n",
            "Accuracy of the network: 99.0 %\n",
            "Accuracy of 0: 99.89795918367346 %\n",
            "Accuracy of 1: 99.64757709251101 %\n",
            "Accuracy of 2: 99.51550387596899 %\n",
            "Accuracy of 3: 99.3069306930693 %\n",
            "Accuracy of 4: 98.26883910386965 %\n",
            "Accuracy of 5: 98.4304932735426 %\n",
            "Accuracy of 6: 99.37369519832986 %\n",
            "Accuracy of 7: 98.73540856031128 %\n",
            "Accuracy of 8: 97.63860369609856 %\n",
            "Accuracy of 9: 99.00891972249752 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 4\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "\n",
        "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "#                                           shuffle=True)\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "#                                          shuffle=False)\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data' , train = True, transform = transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data' , train = False, transform = transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "\n",
        "\n",
        "# classes = ('plane', 'car', 'bird', 'cat',\n",
        "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "print(images.shape)\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)  # Change the number of input channels to 1\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 150)  # Adjust the input size based on the output shape of the previous layer\n",
        "        self.fc2 = nn.Linear(150, 84)\n",
        "        self.fc3 = nn.Linear(84,10)  # num_classes should be set to 10 for MNIST dataset (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -> n, 3, 32, 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "        x = x.view(-1, 16 *4*4)            # -> n, 400\n",
        "        x = F.relu(self.fc1(x))               # -> n, 120\n",
        "        x = F.relu(self.fc2(x))               # -> n, 84\n",
        "        x = self.fc3(x)                       # -> n, 10\n",
        "        return x\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {i}: {acc} %')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import PIL.Image as Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)  # Change the number of input channels to 1\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 200)  # Adjust the input size based on the output shape of the previous layer\n",
        "        self.fc2 = nn.Linear(200, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # num_classes should be set to 10 for MNIST dataset (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "        x = x.view(-1, 16 * 4 * 4)  # -> n, 400\n",
        "        x = F.relu(self.fc1(x))  # -> n, 120\n",
        "        x = F.relu(self.fc2(x))  # -> n, 84\n",
        "        x = self.fc3(x)  # -> n, 10\n",
        "        return x\n",
        "\n",
        "def preprocess_image(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "def predict_digit(image, model):\n",
        "    image = image.to(device)\n",
        "    output = model(image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "# Load and preprocess the image from your phone\n",
        "image_path = [\n",
        "    '/content/0.jpg','/content/0_2.png', '/content/1.jpg',\n",
        "    '/content/2.jpg','/content/2_1.png','/content/3.jpg',\n",
        "    '/content/3.png','/content/4.jpg','/content/5.jpg',\n",
        "    '/content/5_2.jpg','/content/5_3.jpg','/content/6.jpg',\n",
        "    '/content/9_2.png','/content/8.png','/content/4_1.jpg'\n",
        "]\n",
        "corect = [0,0,1,2,2,3,3,4,5,5,5,6,9,8,4]\n",
        "noc = 0\n",
        "nos = 0\n",
        "for i in range(len(corect)):\n",
        "  image = Image.open(image_path[i])\n",
        "  preprocessed_image = preprocess_image(image)\n",
        "\n",
        "  # Make the prediction\n",
        "  predicted_digit = predict_digit(preprocessed_image, model)\n",
        "  print(f'Predicted digit: {predicted_digit}       Actual digit: {corect[i]}')\n",
        "  if (corect[i] == predicted_digit):\n",
        "    noc = noc + 1\n",
        "    print(noc)\n",
        "acc = 100.0 * (noc / len(corect))\n",
        "print(f'Accuracy of the network: {acc} %')"
      ],
      "metadata": {
        "id": "yZuyNSz8469x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}